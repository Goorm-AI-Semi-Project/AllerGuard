{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed031ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.56.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.10.1)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.22.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\MYNOTE\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-vision in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.2)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.34.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-vision) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-vision) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-vision) (3.20.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (2.181.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (4.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.4)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai) (0.30.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\MYNOTE\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate tokenizers sentencepiece\n",
    "!pip install -U google-cloud-vision google-generativeai \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69aeeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 🚀 알레르기 분석 서비스 (GCP Vision API + RAG + LLM Fallback) 시작 ---\n",
      "✅ 표준 알레르기 카테고리 19개 로드 완료.\n",
      "✅ 비-성분 필터 키워드 12개 로드 완료.\n",
      "'distiluse-base-multilingual-cased-v1' 쿼리 임베딩 모델 로드 중...\n",
      "✅ 쿼리 임베딩 모델 로드 완료.\n",
      "Zero-Shot NLI 모델 로드 중 (Fallback 전용)...\n",
      "✅ NLI 모델 로드: joeddav/xlm-roberta-large-xnli\n",
      "GCP Vision API 클라이언트 초기화 중...\n",
      "✅ GCP Vision 클라이언트 준비 완료.\n",
      "사전 계산된 RAG 지식 베이스 로드 중...\n",
      "✅ KB 로드 완료 (항목: 689개, terms:689개)\n",
      "\n",
      "--- LangGraph 워크플로우 빌드 시작 ---\n",
      "--- ✅ LangGraph 워크플로우 컴파일 완료 ---\n",
      "\n",
      "--- [Test Run: GCP OCR + API Parser + RAG + NLI] ---\n",
      "[SELF-CHECK] 중복 임베딩 그룹 수: 0\n",
      "\n",
      "--- (Node 1: call_gcp_vision_api) ---\n",
      "GCP Vision OCR 호출... (이미지: C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\Data\\\\김광무_118.jpg)\n",
      "✅ OCR 성공. 텍스트 길이: 606\n",
      "\n",
      "--- (Node 2: parse_text_via_api) [API Parser] ---\n",
      "✅ Gemini 파싱 완료: queue=26 / pre_found=['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '건당근' (남은 25개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '건당근' → '없음' (유사도 0.7632, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '건양파' (남은 24개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '건양파' → '없음' (유사도 0.7799, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '농축토마토페이스트' (남은 23개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '농축토마토페이스트' → '없음' (유사도 0.7639, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '농축파인애플즙' (남은 22개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '농축파인애플즙' → '없음' (유사도 1.0000, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '농축퓨레' (남은 21개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '농축퓨레' → '없음' (유사도 0.7408, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '닭고기' (남은 20개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '닭고기' → '닭고기' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '닭고기' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '당근' (남은 19개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '당근' → '없음' (유사도 0.7150, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '대두' (남은 18개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '대두' → '대두' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '대두' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '물엿' (남은 17개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '물엿' → '없음' (유사도 0.8205, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '밀' (남은 16개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '밀' → '밀' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '밀' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '발효식초' (남은 15개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '발효식초' → '없음' (유사도 0.7093, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '사과' (남은 14개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '사과' → '없음' (유사도 0.6084, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '사과즙' (남은 13개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '사과즙' → '없음' (유사도 1.0000, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '설탕' (남은 12개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '설탕' → '없음' (유사도 0.6724, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '쇠고기' (남은 11개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '쇠고기' → '쇠고기' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '쇠고기' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '알류' (남은 10개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '알류' → '알류' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '알류' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '양파' (남은 9개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '양파' → '없음' (유사도 0.6768, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '우유' (남은 8개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '우유' → '우유' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '우유' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '전분' (남은 7개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '전분' → '없음' (유사도 0.8101, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '정제소금' (남은 6개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '정제소금' → '없음' (유사도 0.7714, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '정제수' (남은 5개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '정제수' → '없음' (유사도 0.7713, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '조개류' (남은 4개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '조개류' → '조개류' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '조개류' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '천연향신료' (남은 3개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '천연향신료' → '없음' (유사도 0.5284, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '토마토' (남은 2개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '토마토' → '토마토' (유사도 1.0000, by=alias)\n",
      "  -> [RAG 성공] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "✅ 유효 알레르기 추가: '토마토' → ['닭고기', '대두', '밀', '쇠고기', '알류', '우유', '조개류', '토마토']\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '토마토함유' (남은 1개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '토마토함유' → '없음' (유사도 0.9672, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 남음] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "다음 검사 대상: '파인애플' (남은 0개)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG 검색: '파인애플' → '없음' (유사도 0.7872, by=lex_guard)\n",
      "  -> [RAG 결과 없음] update_final_list (폴백 생략)\n",
      "--- (Node 6: update_final_list) ---\n",
      "ℹ️ 표준 알레르기 아님 또는 '없음': '없음' (무시)\n",
      "  -> [항목 없음] finalize_processing\n",
      "\n",
      "--- (Node 7: finalize_processing) ---\n",
      "🎉 최종 결과: [\"닭고기\", \"대두\", \"밀\", \"쇠고기\", \"알류\", \"우유\", \"조개류\", \"토마토\"]\n",
      "\n",
      "최종 반환 JSON:\n",
      "[\"닭고기\", \"대두\", \"밀\", \"쇠고기\", \"알류\", \"우유\", \"조개류\", \"토마토\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "from typing import List, Set, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# GCP Vision OCR\n",
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# (선택) Gemini Structured Output\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    _HAS_GEMINI = True\n",
    "except Exception:\n",
    "    _HAS_GEMINI = False\n",
    "\n",
    "# (선택) Document AI\n",
    "try:\n",
    "    from google.cloud import documentai\n",
    "    _HAS_DOCAI = True\n",
    "except Exception:\n",
    "    _HAS_DOCAI = False\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "print(\"--- 🚀 알레르기 분석 서비스 (GCP Vision API + RAG + LLM Fallback) 시작 ---\")\n",
    "\n",
    "# =====================\n",
    "# 0. 전역 설정/상수\n",
    "# =====================\n",
    "ALLERGENS_STD_SET = set([\n",
    "    \"알류\", \"우유\", \"메밀\", \"땅콩\", \"대두\", \"밀\", \"잣\", \"호두\",\n",
    "    \"게\", \"새우\", \"오징어\", \"고등어\", \"조개류\", \"복숭아\", \"토마토\",\n",
    "    \"닭고기\", \"돼지고기\", \"쇠고기\", \"아황산류\"\n",
    "])\n",
    "print(f\"✅ 표준 알레르기 카테고리 {len(ALLERGENS_STD_SET)}개 로드 완료.\")\n",
    "\n",
    "IGNORE_KEYWORDS = set([\n",
    "    \"열량\", \"탄수화물\", \"단백질\", \"지방\", \"당류\", \"나트륨\", \"콜레스테롤\",\n",
    "    \"포화지방\", \"트랜스지방\", \"내용량\", \"I\", \"II\"\n",
    "])\n",
    "print(f\"✅ 비-성분 필터 키워드 {len(IGNORE_KEYWORDS)}개 로드 완료.\")\n",
    "\n",
    "# 동의어→표준 매핑\n",
    "ALIAS2STD = {\n",
    "    \"난류\": \"알류\", \"계란\": \"알류\", \"달걀\": \"알류\", \"난백\": \"알류\", \"난황\": \"알류\",\n",
    "    \"유청\": \"우유\", \"유청단백\": \"우유\", \"유청단백분말\": \"우유\", \"카제인\": \"우유\", \"카제인나트륨\": \"우유\",\n",
    "    \"치즈\": \"우유\", \"치즈분말\": \"우유\", \"탈지분유\": \"우유\", \"분유\": \"우유\",\n",
    "    \"대두레시틴\": \"대두\", \"레시틴(대두)\": \"대두\", \"밀가루\": \"밀\", \"땅콩버터\": \"땅콩\",\n",
    "    \"호두분태\": \"호두\", \"잣가루\": \"잣\",\n",
    "    \"홍합\": \"조개류\", \"굴\": \"조개류\", \"전복\": \"조개류\",\n",
    "    \"고등어추출물\": \"고등어\", \"새우추출물\": \"새우\", \"오징어먹물\": \"오징어\",\n",
    "    \"복숭아농축액\": \"복숭아\", \"토마토페이스트\": \"토마토\",\n",
    "    \"아황산나트륨\": \"아황산류\",\n",
    "}\n",
    "\n",
    "KB_EMB_PATH = r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\차지예\\\\kb_embeddings.npy\"\n",
    "KB_CAT_PATH = r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\차지예\\\\kb_categories.json\"\n",
    "KB_CSV_PATH = r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\domestic_allergy_rag_knowledge_1000.csv\"\n",
    "\n",
    "KEY_JSON_PATH = os.environ.get(\"GCP_VISION_KEY_PATH\", r\"D:\\key folder\\ocr-project-470906-7ffeebabeb09.json\")\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"distiluse-base-multilingual-cased-v1\"\n",
    "NLI_MODEL_NAME = \"joeddav/xlm-roberta-large-xnli\"\n",
    "\n",
    "USE_API_PARSER = os.environ.get(\"ALLER_GUARD_API_PARSER\", \"gemini\").lower()\n",
    "\n",
    "RAG_CONFIDENCE_THRESHOLD = float(os.environ.get(\"RAG_CONF_THRESH\", 0.85))\n",
    "NLI_FALLBACK_THRESHOLD   = float(os.environ.get(\"NLI_FALLBACK_THRESH\", 0.5))\n",
    "\n",
    "HARDCODED_GEMINI_API_KEY = \"AIzaSyDMTVeVGPU374hlJWEGhxB902f-RxkRVSU\"\n",
    "\n",
    "def _get_gemini_api_key():\n",
    "    if HARDCODED_GEMINI_API_KEY:\n",
    "        return HARDCODED_GEMINI_API_KEY.strip()\n",
    "    for var in (\"GEMINI_API_KEY\", \"GOOGLE_API_KEY\", \"GENAI_API_KEY\"):\n",
    "        v = os.environ.get(var)\n",
    "        if v:\n",
    "            return v\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        for var in (\"GEMINI_API_KEY\", \"GOOGLE_API_KEY\", \"GENAI_API_KEY\"):\n",
    "            v = os.environ.get(var)\n",
    "            if v:\n",
    "                return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    for fname in (\"gemini_api_key.txt\", \".gemini_api_key\"):\n",
    "        if os.path.exists(fname):\n",
    "            try:\n",
    "                with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "                    key = f.read().strip()\n",
    "                    if key:\n",
    "                        return key\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "GENERIC_SUFFIXES = (\n",
    "    \"가루\",\"분말\",\"추출물\",\"농축액\",\"농축분말\",\"유래\",\"단백질\",\"농축\",\n",
    "    \"페이스트\",\"엑기스\",\"분태\",\"시럽\",\"오일\",\"혼합\",\"액\",\"분\",\"정제\",\"가수분해물\"\n",
    ")\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    return x / (np.linalg.norm(x, axis=-1, keepdims=True) + 1e-12)\n",
    "\n",
    "def normalize_to_std(name: str) -> str:\n",
    "    n = re.sub(r\"\\s+\", \"\", str(name))\n",
    "    n = n.split(\"(\")[0]\n",
    "    return ALIAS2STD.get(n, n)\n",
    "\n",
    "def core_token(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"\", str(s))\n",
    "    s = s.split(\"(\")[0]\n",
    "    for suf in GENERIC_SUFFIXES:\n",
    "        if s.endswith(suf) and len(s) > len(suf) + 1:\n",
    "            s = s[:-len(suf)]\n",
    "            break\n",
    "    return s\n",
    "\n",
    "def lexical_consistent(query: str, cand_term: str) -> bool:\n",
    "    q = core_token(query)\n",
    "    c = core_token(cand_term)\n",
    "    if not q or not c:\n",
    "        return False\n",
    "    if q == c:\n",
    "        return True\n",
    "    if len(q) >= 2 and len(c) >= 2 and (q in c or c in q):\n",
    "        return True\n",
    "    return False\n",
    "# =====================\n",
    "# 2. 글로벌 리소스 초기화\n",
    "# =====================\n",
    "try:\n",
    "    print(f\"'{EMBEDDING_MODEL_NAME}' 쿼리 임베딩 모델 로드 중...\")\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(\"✅ 쿼리 임베딩 모델 로드 완료.\")\n",
    "\n",
    "    print(\"Zero-Shot NLI 모델 로드 중 (Fallback 전용)...\")\n",
    "    hf_logging.set_verbosity_error()\n",
    "    try:\n",
    "        import sentencepiece  # noqa: F401\n",
    "    except Exception:\n",
    "        print(\"⚠️ 'sentencepiece' 패키지가 없습니다. 'pip install sentencepiece' 권장(멀티링구얼 모델에 필요)\")\n",
    "\n",
    "    # 안전한 NLI 로더(순차 폴백)\n",
    "    candidates = [\n",
    "        (\"joeddav/xlm-roberta-large-xnli\", False),\n",
    "        (\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\", False),\n",
    "        (\"facebook/bart-large-mnli\", True),  # 영어 전용(긴급 폴백)\n",
    "    ]\n",
    "    last_err = None\n",
    "    nli_pipeline = None\n",
    "    for mid, english_only in candidates:\n",
    "        try:\n",
    "            nli_tokenizer = AutoTokenizer.from_pretrained(mid, use_fast=False)\n",
    "            nli_model = AutoModelForSequenceClassification.from_pretrained(mid)\n",
    "            nli_pipeline = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=nli_model,\n",
    "                tokenizer=nli_tokenizer,\n",
    "                device=(0 if torch.cuda.is_available() else -1),\n",
    "                hypothesis_template=(\n",
    "                    \"이 성분은 {} 알레르겐(과)에 해당한다.\" if not english_only else \"This ingredient belongs to {} allergen.\"\n",
    "                ),\n",
    "            )\n",
    "            NLI_MODEL_NAME = mid\n",
    "            print(f\"✅ NLI 모델 로드: {mid}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ NLI 후보 로드 실패({mid}): {e}\")\n",
    "            last_err = e\n",
    "    if nli_pipeline is None:\n",
    "        raise RuntimeError(f\"NLI 모델 로드 실패(모든 후보 실패): {last_err}\")\n",
    "\n",
    "    # NLI 후보 레이블\n",
    "    ALLERGEN_CANDIDATES = list(ALLERGENS_STD_SET) + [\"관련 없음\"]\n",
    "\n",
    "    # GCP Vision 클라이언트\n",
    "    print(\"GCP Vision API 클라이언트 초기화 중...\")\n",
    "    credentials = service_account.Credentials.from_service_account_file(KEY_JSON_PATH)\n",
    "    vision_client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "    print(\"✅ GCP Vision 클라이언트 준비 완료.\")\n",
    "\n",
    "    # KB 로드 + L2 정규화 + 중복 제거 + 텍스트/용어 매핑\n",
    "    print(\"사전 계산된 RAG 지식 베이스 로드 중...\")\n",
    "\n",
    "    if not os.path.exists(KB_EMB_PATH) or not os.path.exists(KB_CAT_PATH):\n",
    "        raise FileNotFoundError(f\"KB 파일 누락: {KB_EMB_PATH} 또는 {KB_CAT_PATH}\")\n",
    "\n",
    "    kb_embeddings = np.load(KB_EMB_PATH).astype(np.float32)\n",
    "    kb_embeddings = kb_embeddings / (np.linalg.norm(kb_embeddings, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    with open(KB_CAT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        kb_categories = json.load(f)  # 길이 N\n",
    "\n",
    "    # KB terms/texts 확보 (가능하면 CSV에서)\n",
    "    kb_terms, kb_texts = None, None\n",
    "    if os.path.exists(KB_CSV_PATH):\n",
    "        df_kb = pd.read_csv(KB_CSV_PATH)\n",
    "        term_col = \"term\" if \"term\" in df_kb.columns else df_kb.columns[0]\n",
    "        kb_terms = df_kb[term_col].astype(str).tolist()\n",
    "        if \"description\" in df_kb.columns:\n",
    "            kb_texts = (df_kb[term_col].astype(str) + \" | \" + df_kb[\"description\"].astype(str)).tolist()\n",
    "        else:\n",
    "            kb_texts = kb_terms[:]\n",
    "    else:\n",
    "        kb_terms = [f\"item_{i}\" for i in range(len(kb_categories))]\n",
    "        kb_texts = [str(c) for c in kb_categories]\n",
    "\n",
    "    # 임베딩 중복 제거 (해시 기반) → 검색 왜곡 방지\n",
    "    def _dedup_embs(embs: np.ndarray, terms: list, cats: list, texts: list):\n",
    "        import hashlib\n",
    "        seen, keep = {}, []\n",
    "        arr = np.ascontiguousarray(embs)\n",
    "        for i, row in enumerate(arr):\n",
    "            h = hashlib.sha256(row.view(np.uint8)).hexdigest()\n",
    "            if h not in seen:\n",
    "                seen[h] = True\n",
    "                keep.append(i)\n",
    "        return arr[keep], [terms[i] for i in keep], [cats[i] for i in keep], [texts[i] for i in keep]\n",
    "\n",
    "    kb_embeddings, kb_terms, kb_categories, kb_texts = _dedup_embs(\n",
    "        kb_embeddings, kb_terms, kb_categories, kb_texts\n",
    "    )\n",
    "\n",
    "    print(f\"✅ KB 로드 완료 (항목: {len(kb_categories)}개, terms:{len(kb_terms)}개)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 치명적 오류: 글로벌 설정 실패: {e}\")\n",
    "    raise\n",
    "\n",
    "# =====================\n",
    "# 3. 상태 및 노드 타입\n",
    "# =====================\n",
    "class AllergyGraphState(TypedDict):\n",
    "    image_path: str\n",
    "    raw_ocr_text: str\n",
    "    ingredients_to_check: List[str]\n",
    "    current_ingredient: str\n",
    "    rag_result: dict\n",
    "    final_allergens: Set[str]\n",
    "    final_output_json: str\n",
    "\n",
    "# =====================\n",
    "# 4. 노드 구현\n",
    "# =====================\n",
    "# --- Node 1: OCR ---\n",
    "\n",
    "def call_gcp_vision_api(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 1: call_gcp_vision_api) ---\")\n",
    "    img_path = state.get(\"image_path\", \"\")\n",
    "    print(f\"GCP Vision OCR 호출... (이미지: {img_path})\")\n",
    "    if not img_path or not os.path.exists(img_path):\n",
    "        print(\"⚠️ 이미지 경로가 없거나 존재하지 않습니다.\")\n",
    "        return {**state, \"raw_ocr_text\": \"\"}\n",
    "    try:\n",
    "        with io.open(img_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "        response = vision_client.text_detection(image=image)\n",
    "        if response.error.message:\n",
    "            raise RuntimeError(f\"GCP API Error: {response.error.message}\")\n",
    "        raw_text = response.full_text_annotation.text\n",
    "        print(f\"✅ OCR 성공. 텍스트 길이: {len(raw_text)}\")\n",
    "        return {**state, \"raw_ocr_text\": raw_text}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GCP Vision 실패: {e}\")\n",
    "        return {**state, \"raw_ocr_text\": \"\"}\n",
    "\n",
    "\n",
    "# --- API 파서 A: Gemini Structured Output ---\n",
    "\n",
    "def parse_with_gemini_structured(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    raw_text = state.get(\"raw_ocr_text\", \"\")\n",
    "    if not raw_text.strip():\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    if not _HAS_GEMINI:\n",
    "        print(\"⚠️ google-generativeai 미설치. 빈 결과 반환\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    api_key = _get_gemini_api_key()\n",
    "    if not api_key:\n",
    "        print(\"⚠️ Gemini API 키가 없습니다. 환경변수 설정 필요. 빈 결과 반환\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"ingredients_block\": {\"type\": \"string\"},\n",
    "            \"ingredients_list\":  {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"contains_list\":     {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"cross_contamination_lines\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "        },\n",
    "        \"required\": [\"ingredients_block\", \"ingredients_list\", \"contains_list\", \"cross_contamination_lines\"]\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "[역할] 너는 한국 식품표시 전문 감리원.\n",
    "[목표] 아래 OCR 원문에서만 추출하여 JSON으로 반환.\n",
    "\n",
    "[지시]\n",
    "- '원재료명' 블록을 한 덩어리 문자열로 그대로 ingredients_block에 넣어라.\n",
    "- 쉼표/구두점 기준으로 재료를 토큰화한 목록을 ingredients_list에 넣어라.\n",
    "- '알레르기 유발물질', '...함유', '...포함' 등 표시 라인에 등장하는 항목들을 contains_list에 넣어라.\n",
    "- '같은 제조시설/교차오염/혼입 가능' 등 문장을 cross_contamination_lines에 원문 그대로 넣어라.\n",
    "- 원문에 없으면 빈 값/빈 배열을 넣어라. 추측 금지.\n",
    "\n",
    "[OCR 원문]\n",
    "```text\n",
    "{raw_text}\n",
    "```\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": schema,\n",
    "                \"temperature\": 0,\n",
    "            },\n",
    "        )\n",
    "        data = json.loads(resp.text)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Gemini 파서 오류: {e}\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    def _clean(x: str) -> str:\n",
    "        x = re.sub(r\"\\s+\", \"\", x)\n",
    "        x = x.split(\"(\")[0]\n",
    "        return normalize_to_std(x)\n",
    "\n",
    "    ing_list   = [_clean(s) for s in data.get(\"ingredients_list\", []) if s]\n",
    "    contain_ls = [_clean(s) for s in data.get(\"contains_list\", []) if s]\n",
    "\n",
    "    filtered_ing = [i for i in ing_list if i and not any(i.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "    filtered_con = [c for c in contain_ls if c and not any(c.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "\n",
    "    found = set([s for s in filtered_con if s in ALLERGENS_STD_SET])\n",
    "    queue = sorted(set([*filtered_ing, *filtered_con]))\n",
    "\n",
    "    print(f\"✅ Gemini 파싱 완료: queue={len(queue)} / pre_found={sorted(found)}\")\n",
    "    return {**state, \"ingredients_to_check\": queue, \"final_allergens\": found}\n",
    "\n",
    "\n",
    "# --- API 파서 B: Document AI Custom Extractor ---\n",
    "\n",
    "def parse_with_docai(state: AllergyGraphState,\n",
    "                     project_id: str,\n",
    "                     location: str,\n",
    "                     processor_id: str) -> AllergyGraphState:\n",
    "    if not _HAS_DOCAI:\n",
    "        print(\"⚠️ google-cloud-documentai 미설치. 빈 결과 반환\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    img_path = state.get(\"image_path\", \"\")\n",
    "    try:\n",
    "        client = documentai.DocumentProcessorServiceClient()\n",
    "        name = client.processor_path(project=project_id, location=location, processor=processor_id)\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            raw_doc = documentai.RawDocument(content=f.read(), mime_type=\"image/jpeg\")\n",
    "        req = documentai.ProcessRequest(name=name, raw_document=raw_doc)\n",
    "        result = client.process_document(request=req)\n",
    "        doc = result.document\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Document AI 호출 실패: {e}\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    ingredients_block = \"\"\n",
    "    ingredients_list, contains_list, cross_lines = [], [], []\n",
    "\n",
    "    for ent in doc.entities:\n",
    "        t = ent.type_\n",
    "        val = (ent.mention_text or \"\").strip()\n",
    "        if   t == \"ingredients_block\":        ingredients_block = val\n",
    "        elif t == \"ingredients_item\":         ingredients_list.append(val)\n",
    "        elif t == \"allergens_contains_item\":  contains_list.append(val)\n",
    "        elif t == \"cross_contamination_line\": cross_lines.append(val)\n",
    "\n",
    "    def _clean(x: str) -> str:\n",
    "        x = re.sub(r\"\\s+\", \"\", x)\n",
    "        x = x.split(\"(\")[0]\n",
    "        return normalize_to_std(x)\n",
    "\n",
    "    ing_list   = [_clean(s) for s in ingredients_list if s]\n",
    "    contain_ls = [_clean(s) for s in contains_list if s]\n",
    "\n",
    "    filtered_ing = [i for i in ing_list if i and not any(i.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "    filtered_con = [c for c in contain_ls if c and not any(c.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "\n",
    "    found = set([s for s in filtered_con if s in ALLERGENS_STD_SET])\n",
    "    queue = sorted(set([*filtered_ing, *filtered_con]))\n",
    "\n",
    "    print(f\"✅ Document AI 파싱 완료: queue={len(queue)} / pre_found={sorted(found)}\")\n",
    "    return {**state, \"ingredients_to_check\": queue, \"final_allergens\": found}\n",
    "\n",
    "\n",
    "# --- Node 2: API 파서 라우터 ---\n",
    "\n",
    "def parse_text_via_api(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 2: parse_text_via_api) [API Parser] ---\")\n",
    "    if USE_API_PARSER == \"docai\":\n",
    "        project_id = os.environ.get(\"DOCAI_PROJECT\", \"YOUR_GCP_PROJECT\")\n",
    "        location   = os.environ.get(\"DOCAI_LOCATION\", \"asia-northeast1\")\n",
    "        processor  = os.environ.get(\"DOCAI_PROCESSOR_ID\", \"your-processor-id\")\n",
    "        return parse_with_docai(state, project_id, location, processor)\n",
    "    else:\n",
    "        return parse_with_gemini_structured(state)\n",
    "\n",
    "\n",
    "# --- Node 3: 루프 컨트롤러 ---\n",
    "\n",
    "def prepare_next_ingredient(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 3: prepare_next_ingredient) ---\")\n",
    "    queue = list(state.get(\"ingredients_to_check\", []))\n",
    "    if not queue:\n",
    "        print(\"ℹ️ 남은 항목 없음\")\n",
    "        return state\n",
    "    nxt = queue.pop(0)\n",
    "    print(f\"다음 검사 대상: '{nxt}' (남은 {len(queue)}개)\")\n",
    "    return {**state, \"current_ingredient\": nxt, \"ingredients_to_check\": queue}\n",
    "\n",
    "\n",
    "# --- RAG 안전 검색 (top-k + 가드룰) ---\n",
    "\n",
    "def rag_search_topk(query_text: str, k: int = 5, thresh: float = 0.65):\n",
    "    # 0) 동의어→표준: 질의 자체가 표준 알레르겐이면 바로 확정\n",
    "    std = normalize_to_std(query_text)\n",
    "    if std in ALLERGENS_STD_SET:\n",
    "        return [{\"term\": std, \"category\": std, \"text\": std, \"sim\": 1.0, \"found_by\": \"alias\"}]\n",
    "\n",
    "    # 1) 쿼리 임베딩은 항상 새로 계산 (부분 일치 캐시 금지)\n",
    "    q = embedding_model.encode([query_text], normalize_embeddings=True)\n",
    "    q = np.asarray(q, dtype=np.float32)[0]\n",
    "\n",
    "    # 2) 코사인 유사도 (정규화 가정)\n",
    "    sims = kb_embeddings @ q  # (N,)\n",
    "\n",
    "    # 3) top-k\n",
    "    k = max(1, min(k, len(sims)))\n",
    "    top_idx = np.argpartition(-sims, kth=k-1)[:k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        results.append({\n",
    "            \"term\": kb_terms[i],\n",
    "            \"category\": kb_categories[i],\n",
    "            \"text\": kb_texts[i],\n",
    "            \"sim\": float(sims[i]),\n",
    "            \"found_by\": \"rag\"\n",
    "        })\n",
    "\n",
    "    if not results:\n",
    "        return [{\"term\": None, \"category\": \"없음\", \"text\": \"\", \"sim\": 0.0, \"found_by\": \"none\"}]\n",
    "\n",
    "    # 4) 극단값 보정: 0.99 이상인데도 용어가 다르면 살짝 강등\n",
    "    r0 = results[0]\n",
    "    if r0[\"sim\"] >= 0.99:\n",
    "        if normalize_to_std(r0[\"term\"]) != std and r0[\"term\"] != query_text:\n",
    "            r0[\"sim\"] = r0[\"sim\"] - 0.05\n",
    "            results = sorted(results, key=lambda x: -x[\"sim\"])\n",
    "            r0 = results[0]\n",
    "\n",
    "    # 5) **용어 일치성 가드**: 핵심어가 다르면 '없음'으로 차단\n",
    "    if not lexical_consistent(query_text, r0[\"term\"]):\n",
    "        return [{\"term\": None, \"category\": \"없음\", \"text\": \"\", \"sim\": float(r0[\"sim\"]), \"found_by\": \"lex_guard\"}]\n",
    "\n",
    "    # 6) 임계치 미달이면 '없음'\n",
    "    if r0[\"sim\"] < thresh:\n",
    "        return [{\"term\": None, \"category\": \"없음\", \"text\": \"\", \"sim\": float(r0[\"sim\"]), \"found_by\": \"below_thresh\"}]\n",
    "\n",
    "    return results[:k]\n",
    "\n",
    "\n",
    "# --- Node 4: RAG 검색 ---\n",
    "\n",
    "def rag_search(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"--- (Node 4: rag_search) ---\")\n",
    "    ingredient = state.get(\"current_ingredient\", \"\")\n",
    "\n",
    "    cand_list = rag_search_topk(ingredient, k=5, thresh=0.65)\n",
    "    top = cand_list[0]\n",
    "\n",
    "    found = top[\"category\"]\n",
    "    conf  = float(top[\"sim\"])\n",
    "    by    = top.get(\"found_by\")\n",
    "    print(f\"RAG 검색: '{ingredient}' → '{found}' (유사도 {conf:.4f}, by={by})\")\n",
    "\n",
    "    return {**state, \"rag_result\": {\"confidence\": conf, \"found_allergen\": found}}\n",
    "\n",
    "\n",
    "# --- Node 5: LLM Fallback (Zero-Shot) ---\n",
    "\n",
    "def llm_fallback(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"--- (Node 5: llm_fallback) [NLI Zero-Shot] ---\")\n",
    "    ingredient = state.get(\"current_ingredient\", \"\")\n",
    "    try:\n",
    "        resp = nli_pipeline(ingredient, list(ALLERGENS_STD_SET) + [\"관련 없음\"])\n",
    "        top_label, top_score = resp['labels'][0], float(resp['scores'][0])\n",
    "        print(f\"NLI 응답: Label='{top_label}', Score={top_score:.4f}\")\n",
    "        if top_label in ALLERGENS_STD_SET and top_score >= NLI_FALLBACK_THRESHOLD:\n",
    "            return {**state, \"rag_result\": {\"confidence\": top_score, \"found_allergen\": top_label}}\n",
    "        return {**state, \"rag_result\": {\"confidence\": 1.0, \"found_allergen\": \"없음\"}}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ NLI Fallback 오류: {e}\")\n",
    "        return {**state, \"rag_result\": {\"confidence\": 1.0, \"found_allergen\": \"오류\"}}\n",
    "\n",
    "\n",
    "# --- Node 6: 결과 취합 ---\n",
    "\n",
    "def update_final_list(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"--- (Node 6: update_final_list) ---\")\n",
    "    result_allergen = state.get(\"rag_result\", {}).get(\"found_allergen\", \"\")\n",
    "    if result_allergen in ALLERGENS_STD_SET:\n",
    "        s = set(state.get(\"final_allergens\", set()))\n",
    "        s.add(result_allergen)\n",
    "        print(f\"✅ 유효 알레르기 추가: '{result_allergen}' → {sorted(s)}\")\n",
    "        return {**state, \"final_allergens\": s}\n",
    "    print(f\"ℹ️ 표준 알레르기 아님 또는 '없음': '{result_allergen}' (무시)\")\n",
    "    return state\n",
    "\n",
    "\n",
    "# --- Node 7: 종료 ---\n",
    "\n",
    "def finalize_processing(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 7: finalize_processing) ---\")\n",
    "    final_set = set(state.get(\"final_allergens\", set()))\n",
    "    final_list = sorted(list(final_set))\n",
    "    final_json = json.dumps(final_list, ensure_ascii=False)\n",
    "    print(f\"🎉 최종 결과: {final_json}\")\n",
    "    return {**state, \"final_output_json\": final_json}\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 5. 엣지(Edge) 라우터\n",
    "# =====================\n",
    "\n",
    "def route_after_parse(state: AllergyGraphState) -> str:\n",
    "    if state.get(\"ingredients_to_check\"):\n",
    "        return \"has_ingredients\"\n",
    "    return \"no_ingredients\"\n",
    "\n",
    "\n",
    "def route_rag_result(state: AllergyGraphState) -> str:\n",
    "    conf = state.get(\"rag_result\", {}).get(\"confidence\", 0.0)\n",
    "    allergen = state.get(\"rag_result\", {}).get(\"found_allergen\", \"\")\n",
    "\n",
    "    # '없음'이면 폴백 불필요 → 바로 다음 단계로(추가 안 되고 넘어감)\n",
    "    if allergen == \"없음\":\n",
    "        print(\"  -> [RAG 결과 없음] update_final_list (폴백 생략)\")\n",
    "        return \"rag_success\"\n",
    "\n",
    "    if conf >= RAG_CONFIDENCE_THRESHOLD and allergen in ALLERGENS_STD_SET:\n",
    "        print(\"  -> [RAG 성공] update_final_list\")\n",
    "        return \"rag_success\"\n",
    "\n",
    "    print(\"  -> [RAG 불확실] llm_fallback\")\n",
    "    return \"needs_llm_fallback\"\n",
    "\n",
    "\n",
    "def check_remaining_ingredients(state: AllergyGraphState) -> str:\n",
    "    if state.get(\"ingredients_to_check\"):\n",
    "        print(\"  -> [항목 남음] prepare_next_ingredient\")\n",
    "        return \"has_more_ingredients\"\n",
    "    print(\"  -> [항목 없음] finalize_processing\")\n",
    "    return \"all_ingredients_done\"\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 6. 그래프 빌드\n",
    "# =====================\n",
    "print(\"\\n--- LangGraph 워크플로우 빌드 시작 ---\")\n",
    "workflow = StateGraph(AllergyGraphState)\n",
    "\n",
    "# 노드 등록\n",
    "workflow.add_node(\"call_gcp_vision_api\", call_gcp_vision_api)\n",
    "workflow.add_node(\"parse_text_via_api\", parse_text_via_api)\n",
    "workflow.add_node(\"prepare_next_ingredient\", prepare_next_ingredient)\n",
    "workflow.add_node(\"rag_search\", rag_search)\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback)\n",
    "workflow.add_node(\"update_final_list\", update_final_list)\n",
    "workflow.add_node(\"finalize_processing\", finalize_processing)\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.set_entry_point(\"call_gcp_vision_api\")\n",
    "workflow.add_edge(\"call_gcp_vision_api\", \"parse_text_via_api\")\n",
    "\n",
    "# parse → 조건부 분기\n",
    "workflow.add_conditional_edges(\n",
    "    \"parse_text_via_api\",\n",
    "    route_after_parse,\n",
    "    {\"has_ingredients\": \"prepare_next_ingredient\", \"no_ingredients\": \"finalize_processing\"}\n",
    ")\n",
    "\n",
    "# 루프 본체\n",
    "workflow.add_edge(\"prepare_next_ingredient\", \"rag_search\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"rag_search\",\n",
    "    route_rag_result,\n",
    "    {\"rag_success\": \"update_final_list\", \"needs_llm_fallback\": \"llm_fallback\"}\n",
    ")\n",
    "workflow.add_edge(\"llm_fallback\", \"update_final_list\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"update_final_list\",\n",
    "    check_remaining_ingredients,\n",
    "    {\"has_more_ingredients\": \"prepare_next_ingredient\", \"all_ingredients_done\": \"finalize_processing\"}\n",
    ")\n",
    "workflow.add_edge(\"finalize_processing\", END)\n",
    "\n",
    "# 컴파일\n",
    "app = workflow.compile()\n",
    "print(\"--- ✅ LangGraph 워크플로우 컴파일 완료 ---\")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 7. 디버그/검증 유틸 (선택)\n",
    "# =====================\n",
    "\n",
    "def kb_self_check(max_show: int = 5):\n",
    "    \"\"\"중복 임베딩 그룹/샘플 표시\"\"\"\n",
    "    import hashlib\n",
    "    groups = {}\n",
    "    arr = np.ascontiguousarray(kb_embeddings)\n",
    "    for i, row in enumerate(arr):\n",
    "        h = hashlib.sha256(row.view(np.uint8)).hexdigest()\n",
    "        groups.setdefault(h, []).append(i)\n",
    "    dup_groups = {h:idxs for h,idxs in groups.items() if len(idxs) > 1}\n",
    "    print(f\"[SELF-CHECK] 중복 임베딩 그룹 수: {len(dup_groups)}\")\n",
    "    for h, idxs in list(dup_groups.items())[:max_show]:\n",
    "        names = [kb_terms[i] for i in idxs]\n",
    "        cats  = [kb_categories[i] for i in idxs]\n",
    "        print(f\"  - size={len(idxs)} | terms={names[:5]} | cats={cats[:5]}\")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 8. 테스트 실행 (예시)\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- [Test Run: GCP OCR + API Parser + RAG + NLI] ---\")\n",
    "\n",
    "    # (선택) KB 중복 체크\n",
    "    try:\n",
    "        kb_self_check()\n",
    "    except Exception as e:\n",
    "        print(f\"[SELF-CHECK] 실패: {e}\")\n",
    "\n",
    "    # 예시 이미지 경로\n",
    "    test_image = os.environ.get(\"ALLER_GUARD_TEST_IMAGE\", r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\Data\\\\김광무_118.jpg\")\n",
    "    if not os.path.exists(test_image):\n",
    "        print(f\"⚠️ 테스트 이미지가 존재하지 않습니다: {test_image}\")\n",
    "    test_input = {\"image_path\": test_image}\n",
    "\n",
    "    try:\n",
    "        final_state = app.invoke(test_input, {\"recursion_limit\": 1000})\n",
    "        print(\"\\n최종 반환 JSON:\")\n",
    "        print(final_state.get('final_output_json', ''))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 실행 오류: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
